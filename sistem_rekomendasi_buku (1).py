# -*- coding: utf-8 -*-
"""Sistem_Rekomendasi_Buku.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NjSune6BTJ_oeDobimGgMyiDnZPgQE37

# Proyek Pertama (Predictive Analytics):
- **Nama:** Ch Angga Marceclio
- **Email:** chmarcel0603@gmail.com
- **ID Dicoding:** MC315D5Y1131

## 1. Project Overview

### Latar Belakang:

Industri penerbitan buku terus berkembang, dan dengan jutaan buku yang tersedia,
menemukan bacaan yang sesuai selera individu menjadi tantangan tersendiri bagi pembaca. Di sisi lain, penerbit dan penulis juga kesulitan menjangkau audiens yang tepat. Sistem rekomendasi buku hadir sebagai solusi untuk menjembatani kesenjangan ini, membantu pengguna menemukan buku baru yang relevan dan meningkatkan pengalaman membaca mereka.

### Mengapa Proyek Ini Penting:

Proyek ini bertujuan untuk membangun sistem rekomendasi buku yang mampu mempersonalisasi pengalaman pengguna. Dengan merekomendasikan buku berdasarkan preferensi dan riwayat rating pengguna, sistem ini dapat:
- Meningkatkan kepuasan pengguna dengan menyajikan rekomendasi yang relevan.
- Memperluas eksplorasi pengguna terhadap genre dan penulis baru.
- Berpotensi meningkatkan penjualan buku dan keterlibatan pembaca di platform.

## 2. Business Understanding

### Problem Statements:
1. Pengguna seringkali kesulitan menemukan buku yang sesuai dengan minat mereka dari jutaan pilihan yang tersedia.
2. Kurangnya personalisasi dalam penemuan buku dapat menyebabkan pengguna merasa overwhelmed atau melewatkan buku-buku potensial yang akan mereka sukai.

### Goals:
1. Membangun sistem rekomendasi buku yang efektif dan efisien.
2. Memberikan rekomendasi buku yang relevan dan personal kepada pengguna berdasarkan riwayat rating dan preferensi mereka.

### Solution Approach:
Untuk mencapai tujuan ini, kami akan menggunakan pendekatan **Collaborative Filtering**. Pendekatan ini dipilih karena dataset yang tersedia memiliki data rating eksplisit dari pengguna terhadap buku, yang sangat ideal untuk melatih model collaborative filtering. Model ini akan belajar pola preferensi dari interaksi pengguna-item (rating) dan merekomendasikan buku berdasarkan kesamaan preferensi antar pengguna atau kesamaan karakteristik rating antar buku.

### **Import Library**
"""

# Standard Libraries
import re
import string
from io import StringIO
import csv
import pickle
import joblib
import requests

# Utilities
from tqdm import tqdm
from google.colab import drive

# Data Processing
import numpy as np
import pandas as pd

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

# Machine Learning
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline

# Deep Learning
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import (
    Embedding, LSTM, GRU, Conv1D, GlobalMaxPooling1D, Dense, Dropout,
    BatchNormalization, Input, Dot, Flatten
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.models import Model

"""## 3. Data Understanding

Tahap ini bertujuan untuk memahami struktur, karakteristik, dan kualitas data yang akan digunakan. Dimana akan memuat dataset dan melakukan analisis deskriptif serta eksplorasi data awal.

### 3.1. Mount Google Drive & Load Dataset
"""

from google.colab import drive
drive.mount('/content/drive')

base_path = '/content/drive/MyDrive/Colab Notebooks/buku/'

df_books = pd.read_csv(f'{base_path}Books.csv')
df_users = pd.read_csv(f'{base_path}Users.csv')
df_ratings = pd.read_csv(f'{base_path}Ratings.csv')

"""### 3.2. Eksplorasi Data Analysis"""

df_books.head()

df_users.head()

df_ratings.head()

"""### 3.3. Analisis Deskriptif dan Statistik Penting"""

print("Info df_books")
df_books.info()
print("\nInfo df_users")
df_users.info()
print("\nInfo df_ratings")
df_ratings.info()

print("\nDescribe df_books")
print(df_books.describe(include='all'))
print("\nDescribe df_users")
print(df_users.describe(include='all'))
print("\nDescribe df_ratings")
print(df_ratings.describe(include='all'))

print("\nJumlah buku unik:", df_books['Book-Title'].nunique())
print("Jumlah penulis unik:", df_books['Book-Author'].nunique())
print("Jumlah penerbit unik:", df_books['Publisher'].nunique())
print("\nTop 10 Penulis:")
print(df_books['Book-Author'].value_counts().head(10))
print("\nTop 10 Penerbit:")
print(df_books['Publisher'].value_counts().head(10))

"""Output menunjukkan jumlah buku, penulis, dan penerbit unik, serta 10 penulis dan penerbit teratas:

- Terdapat 242,135 buku unik, 102,022 penulis unik, dan 16,807 penerbit unik.
Penulis seperti Agatha Christie, William Shakespeare, dan Stephen King adalah yang paling banyak bukunya.
- Penerbit seperti Harlequin dan Silhouette adalah yang paling banyak bukunya. Insight: Ini menunjukkan skala dan keragaman data. Adanya penulis dan penerbit yang sangat produktif mengindikasikan fenomena "long-tail" yang umum dalam sistem rekomendasi, di mana sebagian kecil item/kreator mendominasi, sementara sebagian besar lainnya memiliki jumlah yang sedikit.
"""

print("\nJumlah User-ID unik:", df_users['User-ID'].nunique())
print("\nDistribusi Usia (Top 10):")
print(df_users['Age'].value_counts(dropna=False).head(10)) # Termasuk NaN

"""Output menunjukkan jumlah User-ID unik dan distribusi usia pengguna:

- Terdapat 278,858 User-ID unik.
- Pada distribusi usia, nilai NaN mendominasi (110,762 entri), diikuti oleh usia 24, 25, dan seterusnya. Insight: Kolom Age memerlukan pembersihan dan imputasi karena banyaknya missing values dan adanya usia yang tidak realistis, sebelum dapat digunakan sebagai fitur yang berarti.
"""

print("\nJumlah rating unik:", df_ratings['Book-Rating'].nunique())
print("\nDistribusi Book-Rating:")
print(df_ratings['Book-Rating'].value_counts().sort_index())

"""Output menunjukkan jumlah rating unik dan distribusi rating buku:

- Terdapat 11 rating unik (dari 0 hingga 10).
- Rating 0 memiliki jumlah terbanyak (716,109 entri), jauh melampaui rating lainnya. Rating 8 dan 10 adalah rating eksplisit positif yang paling sering muncul. Insight: Ini adalah insight paling krusial untuk pemodelan Collaborative Filtering berbasis rating eksplisit. Dominasi rating 0 menegaskan bahwa nilai ini kemungkinan besar adalah implicit feedback atau tidak adanya rating eksplisit, dan perlu difilter keluar untuk model yang mengandalkan preferensi eksplisit (misalnya, pengguna "menyukai" atau "tidak menyukai" buku).

### 3.4. Visualisasi Data (EDA)
"""

# Distribusi Rating
plt.figure(figsize=(8, 5))
sns.countplot(x='Book-Rating', data=df_ratings)
plt.title('Distribusi Rating Buku')
plt.xlabel('Rating')
plt.ylabel('Jumlah')
plt.show()

# Distribusi Usia Pengguna (jika ada nilai yang masuk akal setelah cleaning)
# Anda bisa memfilter usia yang aneh sebelum visualisasi
plt.figure(figsize=(10, 6))
sns.histplot(df_users['Age'].dropna(), bins=30, kde=True)
plt.title('Distribusi Usia Pengguna')
plt.xlabel('Usia')
plt.ylabel('Jumlah Pengguna')
plt.show()

# Top N Penulis
plt.figure(figsize=(12, 6))
df_books['Book-Author'].value_counts().head(10).plot(kind='bar')
plt.title('10 Penulis Terpopuler')
plt.xlabel('Penulis')
plt.ylabel('Jumlah Buku')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""1. Distribusi Rating Buku (Plot):
- Plot ini secara visual menunjukkan bahwa rating 0 memiliki batang yang sangat tinggi dibandingkan dengan rating 1-10. Setelah rating 0, rating 8 dan 10 adalah yang paling sering diberikan.
- Insight: Visualisasi ini memperkuat insight sebelumnya bahwa rating 0 harus ditangani secara terpisah atau dihapus jika sistem rekomendasi hanya berfokus pada preferensi eksplisit. Ini juga menunjukkan bahwa pengguna yang memberikan rating cenderung memberikan nilai yang cukup tinggi.

2. Distribusi Usia Pengguna (Plot):
- Plot histogram menunjukkan distribusi usia pengguna terkonsentrasi pada rentang sekitar 20-an hingga 40-an tahun, dengan puncak di usia 30-an. Terdapat juga pengguna di luar rentang usia normal yang akan ditangani di tahap selanjutnya.
- Insight: Visualisasi ini membantu mengidentifikasi demografi utama pengguna dan kebutuhan untuk membersihkan data usia yang tidak valid agar distribusi menjadi lebih realistis.

3. Top 10 Penulis Terpopuler (Plot)
- Plot bar menunjukkan bahwa Agatha Christie, William Shakespeare, dan Stephen King adalah tiga penulis teratas dengan jumlah buku terbanyak dalam dataset.
- Insight: Plot ini menegaskan adanya "long-tail distribution" pada penulis, di mana beberapa penulis sangat populer dan memiliki banyak buku, sementara sebagian besar penulis lainnya memiliki kontribusi yang jauh lebih sedikit. Ini adalah karakteristik umum data rekomendasi.

## 4. Data Preparation

Tahap ini fokus pada pembersihan, transformasi, dan penggabungan data untuk mempersiapkannya untuk tahap modeling.
"""

# Konversi ke numerik, non-numerik akan menjadi NaN
df_books['Year-Of-Publication'] = pd.to_numeric(df_books['Year-Of-Publication'], errors='coerce')

current_year = 2024
df_books.loc[(df_books['Year-Of-Publication'] > current_year) | (df_books['Year-Of-Publication'] < 1000), 'Year-Of-Publication'] = np.nan

mode_year = df_books['Year-Of-Publication'].mode()[0]
df_books['Year-Of-Publication'].fillna(mode_year, inplace=True)

df_books['Year-Of-Publication'] = df_books['Year-Of-Publication'].astype(int)

print("Kolom 'Year-Of-Publication' berhasil dibersihkan dan diubah ke tipe integer.")
print("Distribusi Tahun Publikasi setelah perbaikan:")
print(df_books['Year-Of-Publication'].value_counts().sort_index(ascending=False).head(10))
print(f"\nInfo df_books setelah perbaikan Year-Of-Publication:")
df_books.info()

"""### 4.1. Penanganan Missing Values"""

df_books.isnull().sum()

df_books.dropna(inplace=True)

df_books.isnull().sum()

df_ratings.isnull().sum()

# Penanganan df_users['Age']
# Mengisi NaN dengan median usia
median_age = df_users['Age'].median()
df_users['Age'].fillna(median_age, inplace=True)

# Memperbaiki usia yang tidak masuk akal (< 5 atau > 100) dengan median
df_users.loc[(df_users['Age'] < 5) | (df_users['Age'] > 100), 'Age'] = median_age
df_users['Age'] = df_users['Age'].astype(int) # Pastikan tipe data integer
print(f"Jumlah NaN di df_users['Age'] setelah imputasi: {df_users['Age'].isnull().sum()}")
print("Kolom 'Age' berhasil dibersihkan dan diubah ke tipe integer.")

# Penanganan df_books['Book-Author'] dan df_books['Publisher']
df_books['Book-Author'].fillna('unknown', inplace=True)
df_books['Publisher'].fillna('unknown', inplace=True)
print("NaN pada 'Book-Author' dan 'Publisher' diisi dengan 'unknown'.")

# 4.3. Penanganan Rating 0 di df_ratings

df_ratings_explicit = df_ratings[df_ratings['Book-Rating'] != 0].copy()
print(f"Jumlah rating sebelum filter rating 0: {len(df_ratings)}")
print(f"Jumlah rating setelah filter rating 0: {len(df_ratings_explicit)}")

# 4.4. Penggabungan DataFrame

print("\n--- Menggabungkan DataFrame (ratings, books, users) ---")
# Gabungkan df_ratings_explicit dengan df_books berdasarkan 'ISBN'
merged_df = pd.merge(df_ratings_explicit, df_books, on='ISBN', how='inner')

# Gabungkan hasilnya dengan df_users berdasarkan 'User-ID'
final_df = pd.merge(merged_df, df_users, on='User-ID', how='inner')

print("\nFinal Merged DataFrame head:")
print(final_df.head())
print("\nFinal Merged DataFrame info:")
final_df.info()
print(f"Shape of final_df: {final_df.shape}")

final_df.shape

"""Penggabungan DataFrame:
- Tiga DataFrame asli (df_ratings_explicit, df_books, df_users) berhasil digabungkan menjadi satu final_df yang komprehensif. DataFrame hasil gabungan ini memiliki 383,837 entri dan 12 kolom.
- Insight: Penggabungan ini menciptakan dataset yang lengkap dan terstruktur, memungkinkan kita untuk mengakses informasi pengguna, buku, dan rating secara bersamaan. Jumlah baris yang dihasilkan menunjukkan jumlah interaksi rating eksplisit yang valid setelah semua pra-pemrosesan.
"""

# 4.6. Mengganti Nama Kolom untuk Konsistensi
print("\n--- Mengganti nama kolom menjadi 'user_id', 'item_id', 'rating' ---")
final_df.rename(columns={'User-ID': 'user_id', 'ISBN': 'item_id', 'Book-Rating': 'rating'}, inplace=True)

# Menampilkan 5 baris pertama dari DataFrame yang sudah bersih dengan nama kolom baru
print("\nFinal DataFrame yang sudah bersih dan siap untuk Modeling (setelah renaming):")
print(final_df.head())

# Encoding User-ID dan ISBN menjadi integer berurutan
user_ids = final_df['user_id'].unique().tolist()
user_to_idx = {user_id: idx for idx, user_id in enumerate(user_ids)}
idx_to_user = {idx: user_id for idx, user_id in enumerate(user_ids)}
final_df['user_encoded'] = final_df['user_id'].map(user_to_idx)

book_ids = final_df['item_id'].unique().tolist()
book_to_idx = {book_id: idx for idx, book_id in enumerate(book_ids)}
idx_to_book = {idx: book_id for idx, book_id in enumerate(book_ids)}
final_df['book_encoded'] = final_df['item_id'].map(book_to_idx)

# Mendapatkan jumlah pengguna dan buku unik
num_users = len(user_to_idx)
num_books = len(book_to_idx)

# Mengonversi rating ke tipe float
final_df['rating'] = final_df['rating'].values.astype(np.float32)

# Mengacak data dan membagi menjadi training dan validation set
from sklearn.model_selection import train_test_split

# Memisahkan fitur dan target
X = final_df[['user_encoded', 'book_encoded']].values
y = final_df['rating'].values

# Split data menjadi training dan validation
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Jumlah pengguna unik: {num_users}")
print(f"Jumlah buku unik: {num_books}")
print(f"Shape X_train: {X_train.shape}")
print(f"Shape X_val: {X_val.shape}")
print(f"Shape y_train: {y_train.shape}")
print(f"Shape y_val: {y_val.shape}")

final_df.head()

"""## 5. Modeling and Result (Collaborative Filtering)"""

# --- Tambahan: Verifikasi Ukuran Final DataFrame ---
print("\n--- Verifikasi Ukuran `final_df` sebelum Modeling ---")
print(f"Shape final_df: {final_df.shape}")
print(f"Jumlah User unik di final_df: {final_df['user_id'].nunique()}")
print(f"Jumlah Buku unik di final_df: {final_df['item_id'].nunique()}")
print(f"Penggunaan memori final_df: {final_df.memory_usage(deep=True).sum() / (1024**2):.2f} MB")

# Ukuran embedding (bisa disesuaikan)
EMBEDDING_SIZE = 50

# Input Layer untuk User dan Book
user_input = Input(shape=(1,), name='user_input')
book_input = Input(shape=(1,), name='book_input')

# Embedding Layer untuk User
user_embedding = Embedding(num_users, EMBEDDING_SIZE, name='user_embedding')(user_input)
user_vec = Flatten(name='user_flatten')(user_embedding)

# Embedding Layer untuk Book
book_embedding = Embedding(num_books, EMBEDDING_SIZE, name='book_embedding')(book_input)
book_vec = Flatten(name='book_flatten')(book_embedding)

# Dot product dari embedding user dan book
prod = Dot(axes=1, name='dot_product')([user_vec, book_vec])

# Model Output
output = Dense(1, activation='linear', name='output_rating')(prod)

# Buat model
model = Model(inputs=[user_input, book_input], outputs=output)

# Compile model
# Menggunakan Adam optimizer dan MSE (Mean Squared Error) sebagai loss function
# Metrik yang digunakan adalah RMSE (Root Mean Squared Error)
model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()])

# Menampilkan ringkasan model
model.summary()

# Pisahkan input untuk model
train_user_encoded = X_train[:, 0]
train_book_encoded = X_train[:, 1]
val_user_encoded = X_val[:, 0]
val_book_encoded = X_val[:, 1]

# Callbacks untuk Early Stopping
early_stopping = EarlyStopping(monitor='val_root_mean_squared_error', patience=5, restore_best_weights=True)

# Latih model
history = model.fit(
    [train_user_encoded, train_book_encoded],
    y_train,
    epochs=20,
    batch_size=64,
    validation_data=([val_user_encoded, val_book_encoded], y_val),
    callbacks=[early_stopping],
    verbose=1
)

"""- Output menampilkan ringkasan arsitektur model. Model ini memiliki total 10,896,102 parameter yang dapat dilatih, sebagian besar berasal dari embedding layer untuk pengguna dan buku. Model dilatih selama 20 epoch dengan batch size 64. Selama pelatihan, val_root_mean_squared_error secara bertahap menurun, mencapai 1.8831 pada epoch terakhir.
- Insight: Arsitektur model menunjukkan implementasi collaborative filtering berbasis embedding, yang mampu menangkap pola tersembunyi dalam interaksi pengguna-buku. Penurunan val_root_mean_squared_error menunjukkan bahwa model berhasil belajar untuk memprediksi rating dengan semakin baik dan mampu menggeneralisasi pada data validasi.
"""

# Evaluasi model pada validation set
loss, rmse = model.evaluate([val_user_encoded, val_book_encoded], y_val, verbose=0)
print(f"\nRMSE pada validation set: {rmse:.4f}")

# Visualisasi Loss dan RMSE
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss History')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['root_mean_squared_error'], label='Training RMSE')
plt.plot(history.history['val_root_mean_squared_error'], label='Validation RMSE')
plt.title('RMSE History')
plt.xlabel('Epoch')
plt.ylabel('RMSE')
plt.legend()
plt.tight_layout()
plt.show()

"""- Hasil evaluasi terakhir menunjukkan RMSE pada validation set sebesar 1.8831. Visualisasi Loss History dan RMSE History menunjukkan tren penurunan yang stabil untuk kedua metrik (loss dan RMSE) baik pada training maupun validation set seiring berjalannya epoch.
- Insight: Nilai RMSE 1.8831 menunjukkan rata-rata selisih prediksi rating dengan rating aktual. Semakin kecil nilai ini, semakin baik modelnya. Plot menunjukkan bahwa model tidak mengalami overfitting yang parah (garis validation mengikuti garis training dengan baik), dan proses pelatihan berhenti pada titik yang optimal berkat EarlyStopping.
"""

# --- Kode sebelumnya (bagian ini sudah benar, hanya untuk konteks) ---
# Mendapatkan daftar semua buku unik
all_book_ids = final_df['item_id'].unique()

# Memilih User ID secara acak untuk mendapatkan rekomendasi
# Pastikan user_id yang dipilih ada di data yang sudah difilter
target_user_id = final_df['user_id'].sample(1).iloc[0]
print(f"\nMencari rekomendasi untuk User-ID: {target_user_id}")

# Mendapatkan buku-buku yang sudah di-rating oleh user ini
books_rated_by_user = final_df[final_df['user_id'] == target_user_id]['item_id'].tolist()

# Mendapatkan buku-buku yang belum di-rating oleh user ini
unrated_books = [book_id for book_id in all_book_ids if book_id not in books_rated_by_user]

# Map buku yang belum di-rating ke encoded ID
unrated_book_encoded = np.array([book_to_idx[book_id] for book_id in unrated_books])

# Buat array user_encoded untuk prediksi (sesuai jumlah unrated_books)
user_encoded_for_pred = np.array([user_to_idx[target_user_id]] * len(unrated_book_encoded))

# Prediksi rating untuk buku-buku yang belum di-rating
predicted_ratings = model.predict([user_encoded_for_pred, unrated_book_encoded]).flatten()

# Buat DataFrame dari hasil prediksi
predictions_df = pd.DataFrame({
    'item_id': unrated_books,
    'predicted_rating': predicted_ratings
})

# Gabungkan dengan metadata buku untuk mendapatkan judul dan penulis
top_recommendations = predictions_df.sort_values(by='predicted_rating', ascending=False).head(10)
# Perbaikan di sini: Ambil metadata dari df_books karena df_books yang berisi semua metadata buku,
# dan pastikan kolom ISBN adalah kolom yang digunakan untuk merge.
top_recommendations = pd.merge(top_recommendations, df_books[['ISBN', 'Book-Title', 'Book-Author', 'Publisher']],
                               left_on='item_id', right_on='ISBN', how='left')

# Tampilkan top 10 rekomendasi
print("\nTop 10 Rekomendasi Buku:")
print(top_recommendations[['Book-Title', 'Book-Author', 'predicted_rating']])

# Tampilkan juga buku yang sudah di-rating oleh user tersebut untuk perbandingan
print(f"\nBuku-buku yang sudah di-rating oleh User-ID {target_user_id} (untuk perbandingan):")
rated_books_info = final_df[final_df['user_id'] == target_user_id] # Ambil baris user_id tertentu dari final_df
# Karena final_df sudah berisi Book-Title dan Book-Author, Anda tidak perlu merge lagi
print(rated_books_info[['Book-Title', 'Book-Author', 'rating']].sort_values(by='rating', ascending=False).head(10))

"""- Untuk User-ID: 243360 (sebagai contoh pengguna), model memprediksi rating untuk buku-buku yang belum di-rating oleh pengguna tersebut. Top 10 rekomendasi buku memiliki predicted rating yang tinggi, berkisar antara 9.5 hingga 10.3. Sebagai perbandingan, buku-buku yang sudah di-rating oleh pengguna ini memiliki rating aktual 8.0 hingga 9.0.
- Insight: Hasil ini menunjukkan bahwa model berhasil menghasilkan rekomendasi buku dengan predicted rating yang tinggi, konsisten dengan kecenderungan pengguna yang suka memberikan rating tinggi pada buku yang sudah dibacanya. Rekomendasi ini bersifat personal dan berpotensi meningkatkan kepuasan pengguna dengan menawarkan buku-buku baru yang sesuai dengan selera mereka.
"""